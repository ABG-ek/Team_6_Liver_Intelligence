{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "g37GinmQ6RMH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import seaborn as sns\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import RobustScaler\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "df_train = pd.read_csv('/content/train.csv')\n",
        "df_test = pd.read_csv('/content/test.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# Определяем категориальные признаки\n",
        "object_columns = df_train.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Инициализируем LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Конвертируем данные типа object  в численные значения:\n",
        "for col in object_columns:\n",
        "    df_train[col] = label_encoder.fit_transform(df_train[col])\n",
        "\n",
        "df_train.head(5).style.set_properties(**{\"background-color\": \"#A8DADC\", \"color\": \"black\", \"border\": \"1.5px solid White\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "bv1RjyNP2leo",
        "outputId": "ccd35b7a-c46d-4d00-f7f4-88a6f6c3f629"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7da2455a1000>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_88e3f_row0_col0, #T_88e3f_row0_col1, #T_88e3f_row0_col2, #T_88e3f_row0_col3, #T_88e3f_row0_col4, #T_88e3f_row0_col5, #T_88e3f_row0_col6, #T_88e3f_row0_col7, #T_88e3f_row0_col8, #T_88e3f_row0_col9, #T_88e3f_row0_col10, #T_88e3f_row0_col11, #T_88e3f_row0_col12, #T_88e3f_row0_col13, #T_88e3f_row0_col14, #T_88e3f_row0_col15, #T_88e3f_row0_col16, #T_88e3f_row0_col17, #T_88e3f_row0_col18, #T_88e3f_row0_col19, #T_88e3f_row1_col0, #T_88e3f_row1_col1, #T_88e3f_row1_col2, #T_88e3f_row1_col3, #T_88e3f_row1_col4, #T_88e3f_row1_col5, #T_88e3f_row1_col6, #T_88e3f_row1_col7, #T_88e3f_row1_col8, #T_88e3f_row1_col9, #T_88e3f_row1_col10, #T_88e3f_row1_col11, #T_88e3f_row1_col12, #T_88e3f_row1_col13, #T_88e3f_row1_col14, #T_88e3f_row1_col15, #T_88e3f_row1_col16, #T_88e3f_row1_col17, #T_88e3f_row1_col18, #T_88e3f_row1_col19, #T_88e3f_row2_col0, #T_88e3f_row2_col1, #T_88e3f_row2_col2, #T_88e3f_row2_col3, #T_88e3f_row2_col4, #T_88e3f_row2_col5, #T_88e3f_row2_col6, #T_88e3f_row2_col7, #T_88e3f_row2_col8, #T_88e3f_row2_col9, #T_88e3f_row2_col10, #T_88e3f_row2_col11, #T_88e3f_row2_col12, #T_88e3f_row2_col13, #T_88e3f_row2_col14, #T_88e3f_row2_col15, #T_88e3f_row2_col16, #T_88e3f_row2_col17, #T_88e3f_row2_col18, #T_88e3f_row2_col19, #T_88e3f_row3_col0, #T_88e3f_row3_col1, #T_88e3f_row3_col2, #T_88e3f_row3_col3, #T_88e3f_row3_col4, #T_88e3f_row3_col5, #T_88e3f_row3_col6, #T_88e3f_row3_col7, #T_88e3f_row3_col8, #T_88e3f_row3_col9, #T_88e3f_row3_col10, #T_88e3f_row3_col11, #T_88e3f_row3_col12, #T_88e3f_row3_col13, #T_88e3f_row3_col14, #T_88e3f_row3_col15, #T_88e3f_row3_col16, #T_88e3f_row3_col17, #T_88e3f_row3_col18, #T_88e3f_row3_col19, #T_88e3f_row4_col0, #T_88e3f_row4_col1, #T_88e3f_row4_col2, #T_88e3f_row4_col3, #T_88e3f_row4_col4, #T_88e3f_row4_col5, #T_88e3f_row4_col6, #T_88e3f_row4_col7, #T_88e3f_row4_col8, #T_88e3f_row4_col9, #T_88e3f_row4_col10, #T_88e3f_row4_col11, #T_88e3f_row4_col12, #T_88e3f_row4_col13, #T_88e3f_row4_col14, #T_88e3f_row4_col15, #T_88e3f_row4_col16, #T_88e3f_row4_col17, #T_88e3f_row4_col18, #T_88e3f_row4_col19 {\n",
              "  background-color: #A8DADC;\n",
              "  color: black;\n",
              "  border: 1.5px solid White;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_88e3f\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_88e3f_level0_col0\" class=\"col_heading level0 col0\" >id</th>\n",
              "      <th id=\"T_88e3f_level0_col1\" class=\"col_heading level0 col1\" >N_Days</th>\n",
              "      <th id=\"T_88e3f_level0_col2\" class=\"col_heading level0 col2\" >Drug</th>\n",
              "      <th id=\"T_88e3f_level0_col3\" class=\"col_heading level0 col3\" >Age</th>\n",
              "      <th id=\"T_88e3f_level0_col4\" class=\"col_heading level0 col4\" >Sex</th>\n",
              "      <th id=\"T_88e3f_level0_col5\" class=\"col_heading level0 col5\" >Ascites</th>\n",
              "      <th id=\"T_88e3f_level0_col6\" class=\"col_heading level0 col6\" >Hepatomegaly</th>\n",
              "      <th id=\"T_88e3f_level0_col7\" class=\"col_heading level0 col7\" >Spiders</th>\n",
              "      <th id=\"T_88e3f_level0_col8\" class=\"col_heading level0 col8\" >Edema</th>\n",
              "      <th id=\"T_88e3f_level0_col9\" class=\"col_heading level0 col9\" >Bilirubin</th>\n",
              "      <th id=\"T_88e3f_level0_col10\" class=\"col_heading level0 col10\" >Cholesterol</th>\n",
              "      <th id=\"T_88e3f_level0_col11\" class=\"col_heading level0 col11\" >Albumin</th>\n",
              "      <th id=\"T_88e3f_level0_col12\" class=\"col_heading level0 col12\" >Copper</th>\n",
              "      <th id=\"T_88e3f_level0_col13\" class=\"col_heading level0 col13\" >Alk_Phos</th>\n",
              "      <th id=\"T_88e3f_level0_col14\" class=\"col_heading level0 col14\" >SGOT</th>\n",
              "      <th id=\"T_88e3f_level0_col15\" class=\"col_heading level0 col15\" >Tryglicerides</th>\n",
              "      <th id=\"T_88e3f_level0_col16\" class=\"col_heading level0 col16\" >Platelets</th>\n",
              "      <th id=\"T_88e3f_level0_col17\" class=\"col_heading level0 col17\" >Prothrombin</th>\n",
              "      <th id=\"T_88e3f_level0_col18\" class=\"col_heading level0 col18\" >Stage</th>\n",
              "      <th id=\"T_88e3f_level0_col19\" class=\"col_heading level0 col19\" >Status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_88e3f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_88e3f_row0_col0\" class=\"data row0 col0\" >0</td>\n",
              "      <td id=\"T_88e3f_row0_col1\" class=\"data row0 col1\" >999</td>\n",
              "      <td id=\"T_88e3f_row0_col2\" class=\"data row0 col2\" >0</td>\n",
              "      <td id=\"T_88e3f_row0_col3\" class=\"data row0 col3\" >21532</td>\n",
              "      <td id=\"T_88e3f_row0_col4\" class=\"data row0 col4\" >1</td>\n",
              "      <td id=\"T_88e3f_row0_col5\" class=\"data row0 col5\" >0</td>\n",
              "      <td id=\"T_88e3f_row0_col6\" class=\"data row0 col6\" >0</td>\n",
              "      <td id=\"T_88e3f_row0_col7\" class=\"data row0 col7\" >0</td>\n",
              "      <td id=\"T_88e3f_row0_col8\" class=\"data row0 col8\" >0</td>\n",
              "      <td id=\"T_88e3f_row0_col9\" class=\"data row0 col9\" >2.300000</td>\n",
              "      <td id=\"T_88e3f_row0_col10\" class=\"data row0 col10\" >316.000000</td>\n",
              "      <td id=\"T_88e3f_row0_col11\" class=\"data row0 col11\" >3.350000</td>\n",
              "      <td id=\"T_88e3f_row0_col12\" class=\"data row0 col12\" >172.000000</td>\n",
              "      <td id=\"T_88e3f_row0_col13\" class=\"data row0 col13\" >1601.000000</td>\n",
              "      <td id=\"T_88e3f_row0_col14\" class=\"data row0 col14\" >179.800000</td>\n",
              "      <td id=\"T_88e3f_row0_col15\" class=\"data row0 col15\" >63.000000</td>\n",
              "      <td id=\"T_88e3f_row0_col16\" class=\"data row0 col16\" >394.000000</td>\n",
              "      <td id=\"T_88e3f_row0_col17\" class=\"data row0 col17\" >9.700000</td>\n",
              "      <td id=\"T_88e3f_row0_col18\" class=\"data row0 col18\" >3.000000</td>\n",
              "      <td id=\"T_88e3f_row0_col19\" class=\"data row0 col19\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_88e3f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_88e3f_row1_col0\" class=\"data row1 col0\" >1</td>\n",
              "      <td id=\"T_88e3f_row1_col1\" class=\"data row1 col1\" >2574</td>\n",
              "      <td id=\"T_88e3f_row1_col2\" class=\"data row1 col2\" >1</td>\n",
              "      <td id=\"T_88e3f_row1_col3\" class=\"data row1 col3\" >19237</td>\n",
              "      <td id=\"T_88e3f_row1_col4\" class=\"data row1 col4\" >0</td>\n",
              "      <td id=\"T_88e3f_row1_col5\" class=\"data row1 col5\" >0</td>\n",
              "      <td id=\"T_88e3f_row1_col6\" class=\"data row1 col6\" >0</td>\n",
              "      <td id=\"T_88e3f_row1_col7\" class=\"data row1 col7\" >0</td>\n",
              "      <td id=\"T_88e3f_row1_col8\" class=\"data row1 col8\" >0</td>\n",
              "      <td id=\"T_88e3f_row1_col9\" class=\"data row1 col9\" >0.900000</td>\n",
              "      <td id=\"T_88e3f_row1_col10\" class=\"data row1 col10\" >364.000000</td>\n",
              "      <td id=\"T_88e3f_row1_col11\" class=\"data row1 col11\" >3.540000</td>\n",
              "      <td id=\"T_88e3f_row1_col12\" class=\"data row1 col12\" >63.000000</td>\n",
              "      <td id=\"T_88e3f_row1_col13\" class=\"data row1 col13\" >1440.000000</td>\n",
              "      <td id=\"T_88e3f_row1_col14\" class=\"data row1 col14\" >134.850000</td>\n",
              "      <td id=\"T_88e3f_row1_col15\" class=\"data row1 col15\" >88.000000</td>\n",
              "      <td id=\"T_88e3f_row1_col16\" class=\"data row1 col16\" >361.000000</td>\n",
              "      <td id=\"T_88e3f_row1_col17\" class=\"data row1 col17\" >11.000000</td>\n",
              "      <td id=\"T_88e3f_row1_col18\" class=\"data row1 col18\" >3.000000</td>\n",
              "      <td id=\"T_88e3f_row1_col19\" class=\"data row1 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_88e3f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_88e3f_row2_col0\" class=\"data row2 col0\" >2</td>\n",
              "      <td id=\"T_88e3f_row2_col1\" class=\"data row2 col1\" >3428</td>\n",
              "      <td id=\"T_88e3f_row2_col2\" class=\"data row2 col2\" >1</td>\n",
              "      <td id=\"T_88e3f_row2_col3\" class=\"data row2 col3\" >13727</td>\n",
              "      <td id=\"T_88e3f_row2_col4\" class=\"data row2 col4\" >0</td>\n",
              "      <td id=\"T_88e3f_row2_col5\" class=\"data row2 col5\" >0</td>\n",
              "      <td id=\"T_88e3f_row2_col6\" class=\"data row2 col6\" >1</td>\n",
              "      <td id=\"T_88e3f_row2_col7\" class=\"data row2 col7\" >1</td>\n",
              "      <td id=\"T_88e3f_row2_col8\" class=\"data row2 col8\" >2</td>\n",
              "      <td id=\"T_88e3f_row2_col9\" class=\"data row2 col9\" >3.300000</td>\n",
              "      <td id=\"T_88e3f_row2_col10\" class=\"data row2 col10\" >299.000000</td>\n",
              "      <td id=\"T_88e3f_row2_col11\" class=\"data row2 col11\" >3.550000</td>\n",
              "      <td id=\"T_88e3f_row2_col12\" class=\"data row2 col12\" >131.000000</td>\n",
              "      <td id=\"T_88e3f_row2_col13\" class=\"data row2 col13\" >1029.000000</td>\n",
              "      <td id=\"T_88e3f_row2_col14\" class=\"data row2 col14\" >119.350000</td>\n",
              "      <td id=\"T_88e3f_row2_col15\" class=\"data row2 col15\" >50.000000</td>\n",
              "      <td id=\"T_88e3f_row2_col16\" class=\"data row2 col16\" >199.000000</td>\n",
              "      <td id=\"T_88e3f_row2_col17\" class=\"data row2 col17\" >11.700000</td>\n",
              "      <td id=\"T_88e3f_row2_col18\" class=\"data row2 col18\" >4.000000</td>\n",
              "      <td id=\"T_88e3f_row2_col19\" class=\"data row2 col19\" >2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_88e3f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_88e3f_row3_col0\" class=\"data row3 col0\" >3</td>\n",
              "      <td id=\"T_88e3f_row3_col1\" class=\"data row3 col1\" >2576</td>\n",
              "      <td id=\"T_88e3f_row3_col2\" class=\"data row3 col2\" >1</td>\n",
              "      <td id=\"T_88e3f_row3_col3\" class=\"data row3 col3\" >18460</td>\n",
              "      <td id=\"T_88e3f_row3_col4\" class=\"data row3 col4\" >0</td>\n",
              "      <td id=\"T_88e3f_row3_col5\" class=\"data row3 col5\" >0</td>\n",
              "      <td id=\"T_88e3f_row3_col6\" class=\"data row3 col6\" >0</td>\n",
              "      <td id=\"T_88e3f_row3_col7\" class=\"data row3 col7\" >0</td>\n",
              "      <td id=\"T_88e3f_row3_col8\" class=\"data row3 col8\" >0</td>\n",
              "      <td id=\"T_88e3f_row3_col9\" class=\"data row3 col9\" >0.600000</td>\n",
              "      <td id=\"T_88e3f_row3_col10\" class=\"data row3 col10\" >256.000000</td>\n",
              "      <td id=\"T_88e3f_row3_col11\" class=\"data row3 col11\" >3.500000</td>\n",
              "      <td id=\"T_88e3f_row3_col12\" class=\"data row3 col12\" >58.000000</td>\n",
              "      <td id=\"T_88e3f_row3_col13\" class=\"data row3 col13\" >1653.000000</td>\n",
              "      <td id=\"T_88e3f_row3_col14\" class=\"data row3 col14\" >71.300000</td>\n",
              "      <td id=\"T_88e3f_row3_col15\" class=\"data row3 col15\" >96.000000</td>\n",
              "      <td id=\"T_88e3f_row3_col16\" class=\"data row3 col16\" >269.000000</td>\n",
              "      <td id=\"T_88e3f_row3_col17\" class=\"data row3 col17\" >10.700000</td>\n",
              "      <td id=\"T_88e3f_row3_col18\" class=\"data row3 col18\" >3.000000</td>\n",
              "      <td id=\"T_88e3f_row3_col19\" class=\"data row3 col19\" >0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_88e3f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_88e3f_row4_col0\" class=\"data row4 col0\" >4</td>\n",
              "      <td id=\"T_88e3f_row4_col1\" class=\"data row4 col1\" >788</td>\n",
              "      <td id=\"T_88e3f_row4_col2\" class=\"data row4 col2\" >1</td>\n",
              "      <td id=\"T_88e3f_row4_col3\" class=\"data row4 col3\" >16658</td>\n",
              "      <td id=\"T_88e3f_row4_col4\" class=\"data row4 col4\" >0</td>\n",
              "      <td id=\"T_88e3f_row4_col5\" class=\"data row4 col5\" >0</td>\n",
              "      <td id=\"T_88e3f_row4_col6\" class=\"data row4 col6\" >1</td>\n",
              "      <td id=\"T_88e3f_row4_col7\" class=\"data row4 col7\" >0</td>\n",
              "      <td id=\"T_88e3f_row4_col8\" class=\"data row4 col8\" >0</td>\n",
              "      <td id=\"T_88e3f_row4_col9\" class=\"data row4 col9\" >1.100000</td>\n",
              "      <td id=\"T_88e3f_row4_col10\" class=\"data row4 col10\" >346.000000</td>\n",
              "      <td id=\"T_88e3f_row4_col11\" class=\"data row4 col11\" >3.650000</td>\n",
              "      <td id=\"T_88e3f_row4_col12\" class=\"data row4 col12\" >63.000000</td>\n",
              "      <td id=\"T_88e3f_row4_col13\" class=\"data row4 col13\" >1181.000000</td>\n",
              "      <td id=\"T_88e3f_row4_col14\" class=\"data row4 col14\" >125.550000</td>\n",
              "      <td id=\"T_88e3f_row4_col15\" class=\"data row4 col15\" >96.000000</td>\n",
              "      <td id=\"T_88e3f_row4_col16\" class=\"data row4 col16\" >298.000000</td>\n",
              "      <td id=\"T_88e3f_row4_col17\" class=\"data row4 col17\" >10.600000</td>\n",
              "      <td id=\"T_88e3f_row4_col18\" class=\"data row4 col18\" >4.000000</td>\n",
              "      <td id=\"T_88e3f_row4_col19\" class=\"data row4 col19\" >0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_train.drop(['Status'], axis=1).select_dtypes(include=[np.number])  # Используем только числовые\n",
        "y = df_train['Status'].values\n",
        "\n",
        "count_y_equals_1 = np.sum(y == 1)\n",
        "count_y_equals_0 = np.sum(y == 0)\n",
        "count_y_equals_2 = np.sum(y == 2)\n",
        "\n",
        "# Вывод количества значений y = 1\n",
        "print(f'Количество значений y = 1: {count_y_equals_1}')\n",
        "print(f'Количество значений y = 0: {count_y_equals_0}')\n",
        "print(f'Количество значений y = 2: {count_y_equals_2}')\n",
        "\n",
        "# Нормализация данных\n",
        "X = (X - X.mean()) / X.std()\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "train_size = int(0.7 * len(X))  # 70% на обучение\n",
        "X_train, X_test = X.values[:train_size], X.values[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "\n",
        "# Определение кастомного класса для логистической регрессии, который содержит методы\n",
        "# для обучения (fit), предсказания (predict) и оценки (evaluate).\n",
        "# Методы класса:\n",
        "# _softmax: вычисляет для входных данных вероятности классов.\n",
        "# fit: выполняет обучение модели, используя градиентный спуск.\n",
        "# predict: возвращает предсказанные классы на основе входящих данных.\n",
        "# evaluate: вычисляет метрики, такие как точность, полнота и F1-score для каждой категории.\n",
        "class LogisticRegressionCustom:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Для численной стабильности\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "        self.weights = np.zeros((num_features, num_classes))\n",
        "        self.bias = np.zeros(num_classes)\n",
        "\n",
        "        for _ in range(self.num_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._softmax(linear_model)\n",
        "\n",
        "            # Преобразуем y в one-hot представление\n",
        "            y_one_hot = np.eye(num_classes)[y]\n",
        "\n",
        "            # Градиентный спуск\n",
        "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y_one_hot))\n",
        "            db = (1 / num_samples) * np.sum(y_predicted - y_one_hot, axis=0)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._softmax(linear_model)\n",
        "        return np.argmax(y_predicted, axis=1)  # Получаем индексы классов с наибольшей вероятностью\n",
        "\n",
        "    def evaluate(self, y_true, y_pred):\n",
        "        metrics = {}\n",
        "        unique_classes = np.unique(y_true)\n",
        "\n",
        "        for label in unique_classes:\n",
        "            TP = np.sum((y_pred == label) & (y_true == label))\n",
        "            TN = np.sum((y_pred != label) & (y_true != label))\n",
        "            FP = np.sum((y_pred == label) & (y_true != label))\n",
        "            FN = np.sum((y_pred != label) & (y_true == label))\n",
        "\n",
        "            accuracy = (TP + TN) / len(y_true)\n",
        "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            metrics[label] = {\n",
        "                'Accuracy': accuracy,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1 Score': f1\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Создаем модель логистической регрессии\n",
        "model = LogisticRegressionCustom()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "metrics = model.evaluate(y_test, predictions)\n",
        "\n",
        "# Вывод метрик для каждого значения Status\n",
        "for label, metric_values in metrics.items():\n",
        "    print(f'Class {label}:')\n",
        "    print('Accuracy:', metric_values['Accuracy'])\n",
        "    print('Precision:', metric_values['Precision'])\n",
        "    print('Recall:', metric_values['Recall'])\n",
        "    print('F1 Score:', metric_values['F1 Score'])\n",
        "    print('---')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRvfe4RZNe0s",
        "outputId": "f2812e5c-4989-43d9-f808-f01f73cbd08b"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество значений y = 1: 275\n",
            "Количество значений y = 0: 4965\n",
            "Количество значений y = 2: 2665\n",
            "Class 0:\n",
            "Accuracy: 0.8145025295109612\n",
            "Precision: 0.8259236067626801\n",
            "Recall: 0.8906144496961512\n",
            "F1 Score: 0.857050032488629\n",
            "---\n",
            "Class 1:\n",
            "Accuracy: 0.9607925801011804\n",
            "Precision: 1.0\n",
            "Recall: 0.010638297872340425\n",
            "F1 Score: 0.021052631578947368\n",
            "---\n",
            "Class 2:\n",
            "Accuracy: 0.822512647554806\n",
            "Precision: 0.7428940568475452\n",
            "Recall: 0.7214554579673776\n",
            "F1 Score: 0.732017823042648\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Посмотрим, какие признаки вносят наибольший вклад в обучение модели и после этого в датасете оставим только лучшие признаки"
      ],
      "metadata": {
        "id": "7eb7tp0MMmgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "\n",
        "# # Предполагается, что df_train уже существует и содержит необходимые данные.\n",
        "\n",
        "# # Добавление новых признаков\n",
        "# df_train['Liver_function'] = df_train['Bilirubin'] / df_train['Albumin']\n",
        "# df_train['Inflammation'] = df_train['Alk_Phos'] / df_train['SGOT']\n",
        "# df_train['Normal_Platelets'] = df_train['Platelets'] / df_train['Stage']\n",
        "# df_train['Normal_Albumin'] = df_train['Albumin'] / (1 + df_train['Edema'])\n",
        "# df_train['Liver_damage'] = df_train['Ascites'] + df_train['Hepatomegaly'] + df_train['Spiders']\n",
        "\n",
        "# # Категоризация возраста\n",
        "# def categorize_age_in_days(age_days):\n",
        "#     if age_days <= 10950:\n",
        "#         return 0  # Young\n",
        "#     elif age_days <= 18250:\n",
        "#         return 1  # Middle_Aged\n",
        "#     elif age_days <= 25550:\n",
        "#         return 2  # Senior\n",
        "#     else:\n",
        "#         return 3  # Elderly\n",
        "\n",
        "# df_train['Age_Group'] = df_train['Age'].apply(categorize_age_in_days)\n",
        "\n",
        "# df_train['High_Risk'] = ((df_train['Bilirubin'] > 2) &\n",
        "#                          (df_train['Copper'] > 140) &\n",
        "#                          (df_train['Alk_Phos'] > 150)).astype(int)\n",
        "\n",
        "# df_train['ALBI'] = (np.log10(df_train['Bilirubin']) * 0.66) - (df_train['Albumin'] * 0.085)\n",
        "\n",
        "# # Подготовка данных\n",
        "# X = df_train.drop(['Status'], axis=1).select_dtypes(include=[np.number])  # Используем только числовые\n",
        "# y = df_train['Status'].values\n",
        "\n",
        "# # Нормализация данных\n",
        "# X = (X - X.mean()) / X.std()\n",
        "\n",
        "# # Разделение данных на обучающую и тестовую выборки\n",
        "# train_size = int(0.8 * len(X))  # 80% на обучение\n",
        "# X_train, X_test = X.values[:train_size], X.values[train_size:]\n",
        "# y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# class LogisticRegressionCustom:\n",
        "#     def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.num_iterations = num_iterations\n",
        "#         self.weights = None\n",
        "#         self.bias = None\n",
        "\n",
        "#     def _sigmoid(self, z):\n",
        "#         return 1 / (1 + np.exp(-z))\n",
        "\n",
        "#     def fit(self, X, y):\n",
        "#         num_samples, num_features = X.shape\n",
        "#         self.weights = np.zeros(num_features)\n",
        "#         self.bias = 0\n",
        "\n",
        "#         for _ in range(self.num_iterations):\n",
        "#             linear_model = np.dot(X, self.weights) + self.bias\n",
        "#             y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "#             # Градиентный спуск\n",
        "#             dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
        "#             db = (1 / num_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "#             self.weights -= self.learning_rate * dw\n",
        "#             self.bias -= self.learning_rate * db\n",
        "\n",
        "#     def predict(self, X):\n",
        "#         linear_model = np.dot(X, self.weights) + self.bias\n",
        "#         y_predicted = self._sigmoid(linear_model)\n",
        "#         y_predicted_class = [1 if i > 0.5 else 0 for i in y_predicted]\n",
        "#         return np.array(y_predicted_class)\n",
        "\n",
        "#     def evaluate(self, y_true, y_pred):\n",
        "#         TP = np.sum((y_pred == 1) & (y_true == 1))\n",
        "#         TN = np.sum((y_pred == 0) & (y_true == 0))\n",
        "#         FP = np.sum((y_pred == 1) & (y_true == 0))\n",
        "#         FN = np.sum((y_pred == 0) & (y_true == 1))\n",
        "\n",
        "#         accuracy = (TP + TN) / len(y_true)\n",
        "#         precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "#         recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "#         f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "#         return accuracy, precision, recall, f1\n",
        "\n",
        "# # Создаем модель\n",
        "# model = LogisticRegressionCustom()\n",
        "# model.fit(X_train, y_train)\n",
        "# predictions = model.predict(X_test)\n",
        "# metrics = model.evaluate(y_test, predictions)\n",
        "\n",
        "# print('Accuracy:', metrics[0])\n",
        "# print('Precision:', metrics[1])\n",
        "# print('Recall:', metrics[2])\n",
        "# print('F1 Score:', metrics[3])\n",
        "\n",
        "# # Вычисление влияния признаков (вывод всех признаков)\n",
        "# feature_importance = np.abs(model.weights)\n",
        "# feature_importance_series = pd.Series(feature_importance, index=X.columns)\n",
        "\n",
        "# # Выводить все признаки с их весами\n",
        "# print(\"Все признаки с весами значимости:\")\n",
        "# print(feature_importance_series)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYrqPFcO9fvm",
        "outputId": "87509974-5aad-4c04-9475-2084a22bcfb9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.523719165085389\n",
            "Precision: 0.14678899082568808\n",
            "Recall: 0.6153846153846154\n",
            "F1 Score: 0.23703703703703705\n",
            "Все признаки с весами значимости:\n",
            "id                  0.823801\n",
            "N_Days              1.242137\n",
            "Drug                0.194111\n",
            "Age                 0.713788\n",
            "Sex                 0.515358\n",
            "Ascites             1.285286\n",
            "Hepatomegaly        1.001728\n",
            "Spiders             0.905116\n",
            "Edema               1.618018\n",
            "Bilirubin           1.794821\n",
            "Cholesterol         0.568670\n",
            "Albumin             0.867511\n",
            "Copper              1.473864\n",
            "Alk_Phos            0.590319\n",
            "SGOT                1.164327\n",
            "Tryglicerides       0.778061\n",
            "Platelets           0.520004\n",
            "Prothrombin         1.764685\n",
            "Stage               1.126129\n",
            "Liver_function      1.812096\n",
            "Inflammation        0.185350\n",
            "Normal_Platelets    0.827217\n",
            "Normal_Albumin      1.559324\n",
            "Liver_damage        1.391704\n",
            "Age_Group           0.517757\n",
            "High_Risk           1.524044\n",
            "ALBI                2.106891\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По весам видно, что как и в предыдущей работе новые признаки имеют существенный вес, даже по сравнению со стандартными, но лучшие признаки отличаются по составу!"
      ],
      "metadata": {
        "id": "kTQ-L6LxM6dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['Liver_function'] = df_train['Bilirubin'] / df_train['Albumin']\n",
        "df_train['Normal_Albumin'] = df_train['Albumin'] / (1 + df_train['Edema'])\n",
        "df_train['Liver_damage'] = df_train['Ascites'] + df_train['Hepatomegaly'] + df_train['Spiders']\n",
        "\n",
        "df_train['High_Risk'] = ((df_train['Bilirubin'] > 2) &\n",
        "                         (df_train['Copper'] > 140) &\n",
        "                         (df_train['Alk_Phos'] > 150)).astype(int)\n",
        "\n",
        "df_train['ALBI'] = (np.log10(df_train['Bilirubin']) * 0.66) - (df_train['Albumin'] * 0.085)\n",
        "\n",
        "# Сохраняем только нужные признаки\n",
        "desired_columns = [\n",
        "    'N_Days', 'Ascites', 'Edema', 'Liver_function', 'ALBI',\n",
        "    'Bilirubin', 'Prothrombin', 'Copper',\n",
        "    'Normal_Albumin', 'Age', 'SGOT', 'High_Risk'\n",
        "]\n",
        "df_train_filtered = df_train[desired_columns + ['Status']]\n",
        "\n",
        "# Разделим данные на признаки и целевую переменную\n",
        "X1 = df_train_filtered.drop(['Status'], axis=1)\n",
        "y1 = df_train_filtered['Status']\n",
        "\n",
        "# Нормализация данных\n",
        "X1 = (X1 - X1.mean()) / X1.std()\n",
        "\n",
        "# Разделение данных на обучающую и тестовую выборки\n",
        "train_size = int(0.7 * len(X))  # 70% на обучение\n",
        "X1_train, X1_test = X1.values[:train_size], X1.values[train_size:]\n",
        "y1_train, y1_test = y1[:train_size], y1[train_size:]\n",
        "\n",
        "class LogisticRegressionCustom1:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Для численной стабильности\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        num_classes = len(np.unique(y))\n",
        "        self.weights = np.zeros((num_features, num_classes))\n",
        "        self.bias = np.zeros(num_classes)\n",
        "\n",
        "        for _ in range(self.num_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._softmax(linear_model)\n",
        "\n",
        "            # Преобразуем y в one-hot представление\n",
        "            y_one_hot = np.eye(num_classes)[y]\n",
        "\n",
        "            # Градиентный спуск\n",
        "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y_one_hot))\n",
        "            db = (1 / num_samples) * np.sum(y_predicted - y_one_hot, axis=0)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self._softmax(linear_model)\n",
        "        return np.argmax(y_predicted, axis=1)  # Получаем индексы классов с наибольшей вероятностью\n",
        "\n",
        "    def evaluate(self, y_true, y_pred):\n",
        "        metrics = {}\n",
        "        unique_classes = np.unique(y_true)\n",
        "\n",
        "        for label in unique_classes:\n",
        "            TP = np.sum((y_pred == label) & (y_true == label))\n",
        "            TN = np.sum((y_pred != label) & (y_true != label))\n",
        "            FP = np.sum((y_pred == label) & (y_true != label))\n",
        "            FN = np.sum((y_pred != label) & (y_true == label))\n",
        "\n",
        "            accuracy = (TP + TN) / len(y_true)\n",
        "            precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "            recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "            metrics[label] = {\n",
        "                'Accuracy': accuracy,\n",
        "                'Precision': precision,\n",
        "                'Recall': recall,\n",
        "                'F1 Score': f1\n",
        "            }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "# Создаем модель\n",
        "model1 = LogisticRegressionCustom1()\n",
        "model1.fit(X1_train, y1_train)\n",
        "predictions1 = model1.predict(X1_test)\n",
        "metrics1 = model1.evaluate(y1_test, predictions1)\n",
        "\n",
        "# Вывод метрик для каждого значения Status\n",
        "for label, metric_values in metrics.items():\n",
        "    print(f'Class {label}:')\n",
        "    print('Accuracy:', metric_values['Accuracy'])\n",
        "    print('Precision:', metric_values['Precision'])\n",
        "    print('Recall:', metric_values['Recall'])\n",
        "    print('F1 Score:', metric_values['F1 Score'])\n",
        "    print('---')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nsfT70CWMtd",
        "outputId": "efb7993e-bfd4-4dbf-a3f6-4735744f3258"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 0:\n",
            "Accuracy: 0.8174536256323778\n",
            "Precision: 0.8283208020050126\n",
            "Recall: 0.8926401080351114\n",
            "F1 Score: 0.8592785180370491\n",
            "---\n",
            "Class 1:\n",
            "Accuracy: 0.9607925801011804\n",
            "Precision: 0.6666666666666666\n",
            "Recall: 0.02127659574468085\n",
            "F1 Score: 0.041237113402061855\n",
            "---\n",
            "Class 2:\n",
            "Accuracy: 0.8204047217537943\n",
            "Precision: 0.7399741267787839\n",
            "Recall: 0.7176913425345044\n",
            "F1 Score: 0.7286624203821657\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По сравнению с датасетом без новых признаков метрики незначительно увеличились, но по классу 1 Precision (доля правильно предсказанных положительных результатов (TP) к общему количеству предсказанных положительных результатов (TP + FP)) упала с 1 до 0,67"
      ],
      "metadata": {
        "id": "aHRrNzW5ZbEj"
      }
    }
  ]
}